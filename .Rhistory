setwd("C:/github/metamix/vignettes")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(metamix)
library(RoBMA)
data("mertens_nudge")
hist(mertens_nudge$cohens_d)
hist(mertens_nudge$cohens_d, breaks = 100)
hist(mertens_nudge$cohens_d, breaks = 100)
mertens_nudge <- subset(mertens_nudge, cohens_d < 1.5)
hist(mertens_nudge$cohens_d, breaks = 100)
mertens_nudge <- subset(mertens_nudge, cohens_d < 1.5)
hist(mertens_nudge$cohens_d, breaks = 100)
#fit models with one-sided selection
fit1_metamix_pb <- sel_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d), M = 1, steps = c(0.5, 0.975), chains = 4, cores = 4)
fit2_metamix_pb <- sel_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d), M = 2, steps = c(0.5, 0.975), chains = 4, cores = 4)
fit3_metamix_pb <- sel_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d), M = 3, steps = c(0.5, 0.975), chains = 4, cores = 4)
ml1_pb = bridge_sampler(fit1_metamix_pb)
library(bridgesampling)
library(bridgesampling)
ml1_pb = bridge_sampler(fit1_metamix_pb)
library(bridgesampling)
ml1_pb = bridge_sampler(fit1_metamix_pb)
ml2_pb = bridge_sampler(fit2_metamix_pb)
ml3_pb = bridge_sampler(fit3_metamix_pb)
# Save all
save(fit1_metamix_pb, fit2_metamix_pb, fit3_metamix_pb,
ml1_pb, ml2_pb, ml3_pb,
file = "metamix_pb_onesided.RData")
fit1_metamix_pb
fit3_metamix_pb
# Fit models
fit1_metamix <- re_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d),
M = 1, chains = 4, cores = 4)
fit2_metamix <- re_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d),
M = 2, chains = 4, cores = 4)
fit3_metamix <- re_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d),
M = 3, chains = 4, cores = 4)
# Compute marginal likelihoods
ml1_nobias <- bridge_sampler(fit1_metamix)
ml2_nobias <- bridge_sampler(fit2_metamix)
ml3_nobias <- bridge_sampler(fit3_metamix)
# Save all
save(fit1_metamix, fit2_metamix, fit3_metamix,
ml1_nobias, ml2_nobias, ml3_nobias,
file = "metamix_nobias.RData")
# Combine all log marginal likelihoods into a named vector
logml_all <- c(
pb_M1      = ml1_pb$logml,
pb_M2      = ml2_pb$logml,
pb_M3      = ml3_pb$logml,
nobias_M1  = ml1_nobias$logml,
nobias_M2  = ml2_nobias$logml,
nobias_M3  = ml3_nobias$logml
)
# Stabilize using log-sum-exp trick
logml_centered <- logml_all - max(logml_all)
unnormalized <- exp(logml_centered)
posterior_probs <- unnormalized / sum(unnormalized)
# Output posterior probabilities
posterior_probs
fit4_metamix_pb <- sel_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d), M = 4, steps = c(0.5, 0.975), chains = 4, cores = 4)
fit4_metamix <- re_mix(y = mertens_nudge$cohens_d, sd = sqrt(mertens_nudge$variance_d),
M = 4, chains = 4, cores = 4)
ml4_pb <- bridge_sampler(fit4_metamix_pb)
ml4_nobias <- bridge_sampler(fit4_metamix)
save(fit4_metamix, fit4_metamix_pb, ml4_pb, ml4_nobias,
file = "4mix.RData")
# Combine all log marginal likelihoods into a named vector
logml_all <- c(
pb_M1      = ml1_pb$logml,
pb_M2      = ml2_pb$logml,
pb_M3      = ml3_pb$logml,
pb_M4      = ml4_pb$logml,
nobias_M1  = ml1_nobias$logml,
nobias_M2  = ml2_nobias$logml,
nobias_M3  = ml3_nobias$logml,
nobias_M4  = ml4_nobias$logml
)
# Stabilize using log-sum-exp trick
logml_centered <- logml_all - max(logml_all)
unnormalized <- exp(logml_centered)
posterior_probs <- unnormalized / sum(unnormalized)
# Output posterior probabilities
posterior_probs
rstan::summary(fit3_metamix_pb, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
rstan::summary(fit4_metamix_pb, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
fit3_metamix_pb
rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
hist(probs)
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
hist(probs, breaks = 100)
fit3_metamix_pb
probs
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
hist(probs, breaks = 100)
rstan::summary(fit3_metamix_pb, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
rstan::summary(fit3_metamix_pb, pars = c("mu", "tau", "omega", "theta"))$summary[ , c("mean", "2.5%", "97.5%")]
rstan::summary(fit4_metamix_pb, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
hist(probs, breaks = 100)
probs
probs <- rstan::summary(fit3_metamix_pb, pars = c("posterior_probs"))$summary[ , c("mean")]
hist(probs, breaks = 100)
(probs > .5)/length(probs)
sum((probs > .5))/length(probs)
setwd("C:/github/metamix")
devtools::build()
library(metamix)
?sim_mix
?sample
#' @param steps p-value cutoffs (one-sided p-values). Currently only supports one or two steps.
#' @param weights relative publication probabilities
#' @param one_sided whether selection is one or two-sided
#' @param thetas mixing weights, defaults to equal weight for each component
#' @param N_low lower bound on primary study sample sizes
#' @param N_high upper bound on primary study sample sizes
#' @param N_shape shape of negative binomial distribution to generate sample sizes (see Maier et al., 2023)
#' @param N_scale scale of negative binomial distribution to generate sample sizes (see Maier et al., 2023)
#' @return A data frame with effect sizes and standard errors
#'
sim_mix <- function(K, M, mu, tau, steps, weights, one_sided, thetas = c(rep(1/M, M)), N_low = 25, N_high = 500, N_shape = 2, N_scale = 58){
#simulate sample sizes from negative binomial distribution (see Maier et al., 2023)
N_seq <- seq(N_low,N_high,1)
N_den <- dnbinom(N_seq, size = N_shape, prob = 1/(N_scale+1) ) /
(pnbinom(N_high, size = N_shape, prob = 1/(N_scale+1) ) - pnbinom(N_low - 1, size = N_shape, prob = 1/(N_scale+1) ))
if(!(length(mu) == M)){
stop("There must be as many means as mixture components.")
}
if(!(length(tau) == M)){
stop("There must be as many taus as mixture components.")
}
if(!(length(weights) == length(steps)+1)){
stop("There must be one weights for every p-value interval.")
}
y <- c()
sds <- c()
while(length(y) < K){
cluster <- sample(1:M, 1, prob = thetas) #select mixture component
delta_i <- rnorm(1, mu[cluster], tau[cluster]) #simulate true effect sizes
n_i <- sample(N_seq, 1, TRUE, N_den)/2 #select sample size using negative binomial density
v_i <- (n_i + n_i)/(n_i*n_i) + delta_i^2/(2*(n_i+n_i)) #Borenstein p.25
sd_i <- sqrt(v_i)
y_i <- rnorm(1, delta_i, sd_i) #simulate empirical effect sizes taking sampling variation into account
##simulate selection
if(one_sided){
crit <- y_i/sd_i
} else {
crit <- abs(y_i/sd_i)
}
I <- findInterval(crit, qnorm(steps)) + 1
if(I == 1){
published <- as.logical(rbinom(1,1,weights[1]))
}
if(I == 2){
published <- as.logical(rbinom(1,1,weights[2]))
}
if(I == 3){
published <- as.logical(rbinom(1,1,weights[3]))
}
if(published == TRUE){
y <- c(y, y_i)
sds <- c(sds, sd_i)
}
}
return(data.frame(y, sds))
}
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(0.5, 0.5))
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(0.5, 0.5), one_sided = FALSE)
dat$y
hist(dat$y)
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(0.25, 0.75), one_sided = FALSE)
hist(dat$y)
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(0.5, 0.75), one_sided = FALSE)
hist(dat$y)
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(1, 1), one_sided = FALSE)
hist(dat$y)
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(1, 1), one_sided = FALSE)
hist(dat$y)
dat <- sim_mix(1000, 2, c(0,1), c(0, 0), steps = c(0.5), weights = c(1, 1), thetas = c(1, 1), one_sided = FALSE)
hist(dat$y)
roxygen2::roxygenize()
roxygen2::roxygenize()
install.packages("../metamix", repos = NULL, type = "source")
install.packages("../metamix", repos = NULL, type = "source")
