res$beta
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.2, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "balance",
costT1T2 = 1,
priorH1H0 = 1,
verbose = FALSE,
printplot = TRUE)
res$alpha
res$beta
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.2, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "balance",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.2, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "minimize",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
## Optimize power for a independent t-test, smallest effect of interest
## d = 0.5, 100 participants per condition
res <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100,
sig.level = x, type = 'two.sample', alternative = 'greater')$power", error = "minimize",
verbose = T,
printplot = T)
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.2, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "minimize",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
warnings()
fit <- pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400, alpha = res$alpha,
alternative = 'superior',
verbose = TRUE)
fit$power
for(i in seq(0, 0.2, 0.01)){
fit <- pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400, alpha = i,
alternative = 'superior',
verbose = TRUE)
print(fit$power)
}
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.2, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "minimize",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "minimize",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
fit <- pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400, alpha = .38,
alternative = 'superior',
verbose = TRUE)
print(fit$power)
fit <- pwrss::pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400, alpha = .38,
alternative = 'superior',
verbose = TRUE)
fit$power
fit$parms
fit$test
fit$power
print(fit$power)
fit
##pwrss package
res <-optimal_alpha(power_function = "pwrss::pwrss.t.reg(beta1 = 0.1, beta0 = 0, margin = 0.05,
k = 3, r2 = 0.30, n=400,
alternative = 'superior',
verbose = FALSE)$power",
error = "minimize",
costT1T2 = 1,
priorH1H0 = 1,
verbose = TRUE,
printplot = TRUE)
res$alpha
res$beta
library(pwr)
library(pwrss)
1-pwr.t.test(d = 0.5, n = 100, sig.level = res$alpha, type = 'two.sample', alternative = 'greater')$power
## Optimize power for a independent t-test, smallest effect of interest
## d = 0.5, 100 participants per condition
res <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100,
sig.level = x, type = 'two.sample', alternative = 'greater')$power", error = "minimize",
verbose = T,
printplot = T)
?optimal_alpha
getwd()
library(rstan)
rstan_options(auto_write = TRUE)
library(foreach)
library(doParallel)
util <- new.env()
source('stan_utility.R', local=util)
1^1/2
reduction <- function(k){
1/2^k
}
reduction(1:100)
reduction <- function(k){
1/2^k/10)
1
reduction(c(5, 10, 15, 20))
reduction <- function(k){
1/2^(k/10)
}
reduction(c(5, 10, 15, 20))
qnorm(3.39)
pnorm(3.39)
install.packages("multicon")
data(lensData)
DIAMONDS.in <- lensData[,32:39] # Self-ratings on 8 Situation Characteristics
library(MetaUtility)
z_to_r(1)
z_to_r(1)
r_to_d
r_to_d(z_to_r(0.7))
z_to_r(0.7)
r_to_d(0.7)
?r_to_d
library(RoBMA)
?d_to_logRR
library(RoBMA)
d2r(1)
d2z(1)
install.packages("installr")
installr::updateR()
library(devtools)
setwd("C:/github/meta_mixtures/papersim_bias")
library(RoBMA)
library(parallel)
library(foreach)
library(metamix)
#parallelize loop
parallel::detectCores()
n.cores <- parallel::detectCores() - 8
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#check cluster definition (optional)
print(my.cluster)
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
#how many workers are available? (optional)
foreach::getDoParWorkers()
tau_combinations <-  list(c(0.05, 0.05), c(0.05, 0.2), c(0.2, 0.05), c(0.2, 0.2))
M_init <- c(2)
mu_1_init <- c(0)#c(0, 0.2, 0.5)
delta_mu_init <- c(0.5, 1) #c(0.25, 0.5, 1)
K_init <- c(100, 300) #400
iter <- 12
theta_combinations <- list(c(0.25, 0.75), c(0.5, 0.5))
weights_combinations <- list(c(0.25, 0.5, 1), c(1, 1, 1))
sim_results <- data.frame(matrix(ncol = 61, nrow = 0))
rounds <- 1
for(tau_index in 1:length(tau_combinations)){
for(M in M_init){
for(mu_1 in mu_1_init){
for(delta_mu in delta_mu_init){
for(K in K_init){
for(theta_index in 1:length(theta_combinations)){
for(weights_index in 1:length(weights_combinations)){
results_i <- foreach(j = 1:iter,
.combine = 'rbind',
.packages = c("rstan", "bridgesampling", "metamix", "metafor", "weightr", "meta")
) %dopar% {
M = 2
tau_index <- 1
theta_index <- 1
mu_1 <- 0
delta_mu = 1
K = 300
weights_index <- 1
tau <- tau_combinations[[tau_index]]
theta  <- theta_combinations[[theta_index]]
weights <- weights_combinations[[weights_index]]
#df_mix <- sim_mix(K, M, mu = c(mu_1), tau = tau[1], steps = c(0.5, 0.95), weights = weights, one_sided = TRUE)
df_mix <- sim_mix(K, M, mu = c(mu_1, mu_1+delta_mu), tau = tau, steps = c(0.5, 0.95), weights = weights, one_sided = TRUE, thetas = theta)
print(df_mix)
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
fit_mix2 <- re_mix(df_mix$y, df_mix$sds, M = 2, cores = 1, chains = 2)
fit_mix3 <- re_mix(df_mix$y, df_mix$sds, M = 3, cores = 1, chains = 2)
sel_mix1 <- sel_mix(df_mix$y, df_mix$sds, M = 1, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix2 <- sel_mix(df_mix$y, df_mix$sds, M = 2, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix3 <- sel_mix(df_mix$y, df_mix$sds, M = 3, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel <- weightfunct(df_mix$y, df_mix$sds^2, steps = c(0.05, 0.5, 1))##exract likelihood ratio test and other estimates for this
model_output <- capture.output(sel)
model_text <- paste(model_output, collapse = "\n")
p_sel <- as.numeric(tail(strsplit(model_text, "\\s+")[[1]], 1))
selmodel(fit_metafor, type = "stepfun", alternative="two.sided", prec = "sei",
steps = c(0.05, 0.5, 1), decreasing=FALSE)
egger <- metabias(df_mix$y, df_mix$sds, method.bias = "Egger")
begg <- metabias(df_mix$y, df_mix$sds, method.bias = "Begg")
tes <- tes(df_mix$y, sei = df_mix$sds)
pet <- lm(y ~ sds, weights = 1/sds^2, data = df_mix)
pet <- summary(pet) #if significant
#puni <- puni_star(yi = df_mix$y, vi = df_mix$sds^2, side = "right", alpha = .10, method = "ML")
peese <- lm(y ~ I(sds^2), weights = 1/sds^2, data = df_mix)
peese <- summary(peese)
logml_1 <- bridge_sampler(fit_mix1)$logml
logml_2 <- bridge_sampler(fit_mix2)$logml
logml_3 <- bridge_sampler(fit_mix3)$logml
logml_sel_1 <- bridge_sampler(sel_mix1)$logml
logml_sel_2 <- bridge_sampler(sel_mix2)$logml
logml_sel_3 <- bridge_sampler(sel_mix3)$logml
logml <- c(logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3)
exp(logml)/sum(exp(logml))
fit_metafor <- rma(y, sds^2, data = df_mix)
tau_re <- confint(fit_metafor)$random[2,]
#summary(flexmix(df_mix$y ~ 1, k = 3))
#summary(flexmix(df_mix$y ~ 1, k = 2))
post_means <- summary(fit_mix2, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
post_means_sel <- summary(sel_mix2, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
#post_means_brme <- summary(fit_mix1, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
return(c(mu_1, delta_mu, tau, K, M, theta, logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3,
as.vector(t(as.matrix(post_means))), as.vector(t(as.matrix(post_means_sel))),
sel_model$adj_est[2], sel_model$adj_se[2], p_sel, egger$p.value, begg$p.value, tes$pval,
pet$coefficients[1,1], pet$coefficients[1,2], pet$coefficients[1,4], pet$coefficients[2,4],
peese$coefficients[1,1], peese$coefficients[1,2], peese$coefficients[1,4], peese$coefficients[2,4],
fit_metafor$b, fit_metafor$ci.lb, fit_metafor$ci.ub, tau_re,
as.vector(t(as.matrix(post_means_brme)))[1:3], as.vector(t(as.matrix(post_means_brme)))[1:3]))## need to edit this!!
}
sim_results <- rbind(sim_results, results_i)
print(rounds)
rounds = rounds + 1
}
}
}
}
}
}
}
setwd("C:/github/meta_mixtures/papersim_bias")
library(RoBMA)
library(parallel)
library(foreach)
library(metamix)
#parallelize loop
parallel::detectCores()
n.cores <- parallel::detectCores() - 8
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#check cluster definition (optional)
print(my.cluster)
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
#how many workers are available? (optional)
foreach::getDoParWorkers()
tau_combinations <-  list(c(0.05, 0.05), c(0.05, 0.2), c(0.2, 0.05), c(0.2, 0.2))
M_init <- c(2)
mu_1_init <- c(0)#c(0, 0.2, 0.5)
delta_mu_init <- c(0.5, 1) #c(0.25, 0.5, 1)
K_init <- c(100, 300) #400
iter <- 12
theta_combinations <- list(c(0.25, 0.75), c(0.5, 0.5))
weights_combinations <- list(c(0.25, 0.5, 1), c(1, 1, 1))
sim_results <- data.frame(matrix(ncol = 61, nrow = 0))
rounds <- 1
for(tau_index in 1:length(tau_combinations)){
for(M in M_init){
for(mu_1 in mu_1_init){
for(delta_mu in delta_mu_init){
for(K in K_init){
for(theta_index in 1:length(theta_combinations)){
for(weights_index in 1:length(weights_combinations)){
results_i <- foreach(j = 1:iter,
.combine = 'rbind',
.packages = c("rstan", "bridgesampling", "metamix", "metafor", "weightr", "meta")
) %dopar% {
M = 2
tau_index <- 1
theta_index <- 1
mu_1 <- 0
delta_mu = 1
K = 300
weights_index <- 1
tau <- tau_combinations[[tau_index]]
theta  <- theta_combinations[[theta_index]]
weights <- weights_combinations[[weights_index]]
#df_mix <- sim_mix(K, M, mu = c(mu_1), tau = tau[1], steps = c(0.5, 0.95), weights = weights, one_sided = TRUE)
df_mix <- sim_mix(K, M, mu = c(mu_1, mu_1+delta_mu), tau = tau, steps = c(0.5, 0.95), weights = weights, one_sided = TRUE, thetas = theta)
print(df_mix)
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
fit_mix2 <- re_mix(df_mix$y, df_mix$sds, M = 2, cores = 1, chains = 2)
fit_mix3 <- re_mix(df_mix$y, df_mix$sds, M = 3, cores = 1, chains = 2)
sel_mix1 <- sel_mix(df_mix$y, df_mix$sds, M = 1, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix2 <- sel_mix(df_mix$y, df_mix$sds, M = 2, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix3 <- sel_mix(df_mix$y, df_mix$sds, M = 3, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel <- weightfunct(df_mix$y, df_mix$sds^2, steps = c(0.05, 0.5, 1))##exract likelihood ratio test and other estimates for this
model_output <- capture.output(sel)
model_text <- paste(model_output, collapse = "\n")
p_sel <- as.numeric(tail(strsplit(model_text, "\\s+")[[1]], 1))
egger <- metabias(df_mix$y, df_mix$sds, method.bias = "Egger")
begg <- metabias(df_mix$y, df_mix$sds, method.bias = "Begg")
tes <- tes(df_mix$y, sei = df_mix$sds)
pet <- lm(y ~ sds, weights = 1/sds^2, data = df_mix)
pet <- summary(pet) #if significant
#puni <- puni_star(yi = df_mix$y, vi = df_mix$sds^2, side = "right", alpha = .10, method = "ML")
peese <- lm(y ~ I(sds^2), weights = 1/sds^2, data = df_mix)
peese <- summary(peese)
logml_1 <- bridge_sampler(fit_mix1)$logml
logml_2 <- bridge_sampler(fit_mix2)$logml
logml_3 <- bridge_sampler(fit_mix3)$logml
logml_sel_1 <- bridge_sampler(sel_mix1)$logml
logml_sel_2 <- bridge_sampler(sel_mix2)$logml
logml_sel_3 <- bridge_sampler(sel_mix3)$logml
logml <- c(logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3)
exp(logml)/sum(exp(logml))
fit_metafor <- rma(y, sds^2, data = df_mix)
tau_re <- confint(fit_metafor)$random[2,]
#summary(flexmix(df_mix$y ~ 1, k = 3))
#summary(flexmix(df_mix$y ~ 1, k = 2))
post_means <- summary(fit_mix2, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
post_means_sel <- summary(sel_mix2, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
#post_means_brme <- summary(fit_mix1, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
return(c(mu_1, delta_mu, tau, K, M, theta, logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3,
as.vector(t(as.matrix(post_means))), as.vector(t(as.matrix(post_means_sel))),
sel_model$adj_est[2], sel_model$adj_se[2], p_sel, egger$p.value, begg$p.value, tes$pval,
pet$coefficients[1,1], pet$coefficients[1,2], pet$coefficients[1,4], pet$coefficients[2,4],
peese$coefficients[1,1], peese$coefficients[1,2], peese$coefficients[1,4], peese$coefficients[2,4],
fit_metafor$b, fit_metafor$ci.lb, fit_metafor$ci.ub, tau_re,
as.vector(t(as.matrix(post_means_brme)))[1:3], as.vector(t(as.matrix(post_means_brme)))[1:3]))## need to edit this!!
}
sim_results <- rbind(sim_results, results_i)
print(rounds)
rounds = rounds + 1
}
}
}
}
}
}
}
setwd("C:/github/meta_mixtures/papersim_bias")
library(RoBMA)
library(parallel)
library(foreach)
library(metamix)
#parallelize loop
parallel::detectCores()
n.cores <- parallel::detectCores() - 8
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#check cluster definition (optional)
print(my.cluster)
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
#how many workers are available? (optional)
foreach::getDoParWorkers()
tau_combinations <-  list(c(0.05, 0.05), c(0.05, 0.2), c(0.2, 0.05), c(0.2, 0.2))
M_init <- c(2)
mu_1_init <- c(0)#c(0, 0.2, 0.5)
delta_mu_init <- c(0.5, 1) #c(0.25, 0.5, 1)
K_init <- c(100, 300) #400
iter <- 12
theta_combinations <- list(c(0.25, 0.75), c(0.5, 0.5))
weights_combinations <- list(c(0.25, 0.5, 1), c(1, 1, 1))
sim_results <- data.frame(matrix(ncol = 61, nrow = 0))
rounds <- 1
for(tau_index in 1:length(tau_combinations)){
for(M in M_init){
for(mu_1 in mu_1_init){
for(delta_mu in delta_mu_init){
for(K in K_init){
for(theta_index in 1:length(theta_combinations)){
for(weights_index in 1:length(weights_combinations)){
results_i <- foreach(j = 1:iter,
.combine = 'rbind',
.packages = c("rstan", "bridgesampling", "metamix", "metafor", "weightr", "meta")
) %dopar% {
M = 2
tau_index <- 1
theta_index <- 1
mu_1 <- 0
delta_mu = 1
K = 300
weights_index <- 1
tau <- tau_combinations[[tau_index]]
theta  <- theta_combinations[[theta_index]]
weights <- weights_combinations[[weights_index]]
#df_mix <- sim_mix(K, M, mu = c(mu_1), tau = tau[1], steps = c(0.5, 0.95), weights = weights, one_sided = TRUE)
df_mix <- sim_mix(K, M, mu = c(mu_1, mu_1+delta_mu), tau = tau, steps = c(0.5, 0.95), weights = weights, one_sided = TRUE, thetas = theta)
print(df_mix)
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
fit_mix2 <- re_mix(df_mix$y, df_mix$sds, M = 2, cores = 1, chains = 2)
fit_mix3 <- re_mix(df_mix$y, df_mix$sds, M = 3, cores = 1, chains = 2)
sel_mix1 <- sel_mix(df_mix$y, df_mix$sds, M = 1, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix2 <- sel_mix(df_mix$y, df_mix$sds, M = 2, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_mix3 <- sel_mix(df_mix$y, df_mix$sds, M = 3, steps = c(0.5, 0.95), cores = 1, chains = 2)
sel_model <- weightfunct(df_mix$y, df_mix$sds^2, steps = c(0.05, 0.5, 1))##exract likelihood ratio test and other estimates for this
model_output <- capture.output(sel)
model_text <- paste(model_output, collapse = "\n")
p_sel <- as.numeric(tail(strsplit(model_text, "\\s+")[[1]], 1))
egger <- metabias(df_mix$y, df_mix$sds, method.bias = "Egger")
begg <- metabias(df_mix$y, df_mix$sds, method.bias = "Begg")
tes <- tes(df_mix$y, sei = df_mix$sds)
pet <- lm(y ~ sds, weights = 1/sds^2, data = df_mix)
pet <- summary(pet) #if significant
#puni <- puni_star(yi = df_mix$y, vi = df_mix$sds^2, side = "right", alpha = .10, method = "ML")
peese <- lm(y ~ I(sds^2), weights = 1/sds^2, data = df_mix)
peese <- summary(peese)
logml_1 <- bridge_sampler(fit_mix1)$logml
logml_2 <- bridge_sampler(fit_mix2)$logml
logml_3 <- bridge_sampler(fit_mix3)$logml
logml_sel_1 <- bridge_sampler(sel_mix1)$logml
logml_sel_2 <- bridge_sampler(sel_mix2)$logml
logml_sel_3 <- bridge_sampler(sel_mix3)$logml
logml <- c(logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3)
exp(logml)/sum(exp(logml))
fit_metafor <- rma(y, sds^2, data = df_mix)
tau_re <- confint(fit_metafor)$random[2,]
#summary(flexmix(df_mix$y ~ 1, k = 3))
#summary(flexmix(df_mix$y ~ 1, k = 2))
post_means <- summary(fit_mix2, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
post_means_sel <- summary(sel_mix2, pars = c("mu", "tau", "omega"))$summary[ , c("mean", "2.5%", "97.5%")]
#post_means_brme <- summary(fit_mix1, pars = c("mu", "tau"))$summary[ , c("mean", "2.5%", "97.5%")]
return(c(mu_1, delta_mu, tau, K, M, theta, logml_1, logml_2, logml_3, logml_sel_1, logml_sel_2, logml_sel_3,
as.vector(t(as.matrix(post_means))), as.vector(t(as.matrix(post_means_sel))),
sel_model$adj_est[2], sel_model$adj_se[2], p_sel, egger$p.value, begg$p.value, tes$pval,
pet$coefficients[1,1], pet$coefficients[1,2], pet$coefficients[1,4], pet$coefficients[2,4],
peese$coefficients[1,1], peese$coefficients[1,2], peese$coefficients[1,4], peese$coefficients[2,4],
fit_metafor$b, fit_metafor$ci.lb, fit_metafor$ci.ub, tau_re,
as.vector(t(as.matrix(post_means_brme)))[1:3], as.vector(t(as.matrix(post_means_brme)))[1:3]))## need to edit this!!
}
sim_results <- rbind(sim_results, results_i)
print(rounds)
rounds = rounds + 1
}
}
}
}
}
}
}
?foreach
library(weightr)
?weightfunct
devtools::build()
setwd("C:/github/metamix/inst/stan")
devtools::build()
devtools::build(vignettes = FALSE)
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
library(metamix)
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
y_predict <- as.matrix(fit_mix1, pars = "y_rep")
remove.packages("metamix")
devtools::load_all()
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
y_predict <- as.matrix(fit_mix1, pars = "y_rep")
devtools::document()
devtools::load_all()
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
y_predict <- as.matrix(fit_mix1, pars = "y_rep")
getwd()
# Remove the compiled Stan models
pkgbuild::clean_dll()
# This will recompile all Stan models in inst/stan
pkgbuild::compile_dll()
devtools::load_all()
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
fit_mix1 <- re_mix(df_mix$y, df_mix$sds, M = 1, cores = 1, chains = 2)
y_predict <- as.matrix(fit_mix1, pars = "y_rep")
# Document and reinstall
devtools::document()
df_mix <- sim_mix(300, 2, mu = c(0,1), tau = c(0.2, 0.2), steps = c(0.5, 0.95), weights = c(1, 1, 1), one_sided = TRUE, thetas = c(0.5, 0.5))
library(metamix)
devtools::build(vignettes = FALSE)
devtools::load_all()
